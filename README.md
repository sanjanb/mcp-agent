# HR Agent (Model Context Protocol)

A minimalist HR Assistant that answers policy questions from your own documents. It retrieves the most relevant sections from a vector database and generates grounded responses with citations using OpenAI or Gemini. When no LLM is configured, it falls back to a reliable Basic (retrievalâ€‘only) mode.

Although this is a simple project, the architecture and delivery plan follow industryâ€‘level patterns and are deliberately modular and flexible, making it easy to integrate all features discussed in the plan (`docs/plan.md`).

## Features

Quick Links: [Quick Start](#quick-start-windows-powershell) Â· [Architecture](#architecture-overview) Â· [How It Works](docs/HOW_IT_WORKS.md) Â· [HR Policy RAG Feature](docs/FEATURE_HR_POLICY_RAG.md) Â· [Project Plan](docs/plan.md)

- Grounded answers with explicit citations (RAG)
- Multiâ€‘provider: OpenAI, Gemini, or Auto (prefer â†’ fallback)
- Lowâ€‘latency mode (faster models, fewer tokens, lower temperature)
- Conversation summary caching per session (Redis or inâ€‘memory)
- Minimal Streamlit UI with sticky input and streaming responses
- Works even without LLM keys (retrievalâ€‘only Basic mode)

## Architecture Overview

The UI calls an MCP Server that routes to a Policy RAG tool. The tool searches a vector database (Chroma) for relevant chunks and the RAG Engine composes a prompt for the selected LLM provider (or uses retrievalâ€‘only fallback). Conversation summaries are cached to shrink context on followâ€‘ups.

- Full details: `docs/HOW_IT_WORKS.md`
- Feature guide: `docs/FEATURE_HR_POLICY_RAG.md`

```mermaid
flowchart LR
   subgraph UI
      ST["Streamlit Chat UI"]
   end

   subgraph MCP["MCP Server & Router"]
      Router[Router / Health]
   end

   subgraph TOOLS["Policy RAG Tool"]
      PS[PolicySearchTool]
      RAG[RAGEngine]
      CONV[ConversationManager]
      CACHE[(Redis / Inâ€‘Memory Cache)]
   end

   subgraph DATA["Data Stores"]
      CHROMA[(Chroma Vector DB)]
      DOCS[(HR Documents)]
   end

   subgraph LLM["Providers"]
      OPENAI[OpenAI]
      GEMINI[Gemini]
   end

   ST --> Router
   Router --> PS
   Router --> RAG
   PS --> CHROMA
   PS --> DOCS
   RAG --> OPENAI
   RAG --> GEMINI
   CONV --> CACHE
```

## Prerequisites

- Python 3.8+
- Optional: OpenAI or Gemini API key (for AI responses)
- Optional: Redis (for crossâ€‘process summary caching)

## Quick Start (Windows PowerShell)

```powershell
# 1) Create and activate a virtual environment
python -m venv .venv; .\.venv\Scripts\activate

# 2) Install dependencies
pip install -r requirements.txt

# 3) (Optional) Configure provider for AI responses
set OPENAI_API_KEY=your-openai-key
# or
set GEMINI_API_KEY=your-gemini-key

# 4) Ingest documents (place PDFs/text under data/hr_documents/ first)
python setup.py

# 5) (Optional) Warm-up to reduce first-response latency
python scripts/warmup.py

# 6) Start the UI
streamlit run ui/streamlit_app.py
```

## Configuration (Environment)

Core settings:

```bash
# Providers (optional, enables AI responses)
OPENAI_API_KEY=...
GEMINI_API_KEY=...
GEMINI_MODEL=gemini-1.5-pro

# Vector DB & documents
VECTOR_DB_PATH=./data/vector_db
VECTOR_DB_COLLECTION_NAME=hr_policies
HR_DOCUMENTS_PATH=./data/hr_documents
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Redis cache (optional; falls back to in-memory)
REDIS_URL=redis://localhost:6379/0
# or REDIS_HOST, REDIS_PORT, REDIS_DB

# Low-latency tuning (optional)
FAST_OPENAI_MODEL=gpt-3.5-turbo
FAST_GEMINI_MODEL=gemini-1.5-flash
LOW_LATENCY_MAX_TOKENS=350
LOW_LATENCY_TEMPERATURE=0.0
```

## Usage

- Ask questions like: â€œHow many vacation days do I get?â€, â€œWhatâ€™s the remote work policy?â€
- Commands in the input:
  - `/clear` â€” reset chat
  - `/help` â€” list commands
  - `/provider openai|gemini|auto` â€” switch provider at runtime
- Settings (sidebar â†’ Settings):
  - Search results (Topâ€‘K)
  - Lowâ€‘latency mode (prefer faster models or Basic)
  - Fast responses (smaller retrieval context for speed)
- Sources are shown beneath answers; retrievalâ€‘only mode indicates documentâ€‘based answers without an LLM.

## Performance

This project is optimized for fast startup and quick replies:

- Warmâ€‘up script (`scripts/warmup.py`) primes vector search and the LLM provider
- Lowâ€‘latency mode caps tokens, lowers temperature, and uses faster models
- Conversation summary caching (Redis or inâ€‘memory) shrinks prompts on followâ€‘ups
- â€œFast responsesâ€ limits Topâ€‘K to reduce retrieval/LLM input size
- Streaming renderer shows results progressively for snappy UX

## Caching

- Perâ€‘session conversation summaries are cached at key `conv:summary:<user_id>` (â‰ˆ30 minutes)
- Uses Redis if available; otherwise a perâ€‘process inâ€‘memory fallback
- Summaries are combined with only the most recent turns to reduce LLM prompt size

## Project Structure

```
mcp-agent/
â”œâ”€â”€ mcp_server/             # MCP server and router
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ policy_rag/
â”‚   â”‚   â”œâ”€â”€ document_processor.py
â”‚   â”‚   â”œâ”€â”€ vector_database.py
â”‚   â”‚   â”œâ”€â”€ mcp_tool.py              # PolicySearchTool (search/stats)
â”‚   â”‚   â””â”€â”€ rag_engine.py            # RAGEngine + ConversationManager
â”‚   â””â”€â”€ cache/
â”‚       â””â”€â”€ redis_cache.py           # Redis or in-memory JSON cache
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py             # Minimal chat UI
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ warmup.py                    # Startup warm-up
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ hr_documents/                # Your PDFs/text
â”‚   â””â”€â”€ vector_db/                   # Chroma index
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ HOW_IT_WORKS.md              # Architecture & data flow
â”‚   â””â”€â”€ FEATURE_HR_POLICY_RAG.md     # Feature guide
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup.py
```

## Troubleshooting

- No AI responses: set `OPENAI_API_KEY` or `GEMINI_API_KEY`; the app will otherwise use Basic mode
- No documents found: add files to `data/hr_documents/` and run `python setup.py`
- Slow first response: run `python scripts/warmup.py` and enable Lowâ€‘latency mode
- Cache warnings: if Redis is not reachable, inâ€‘memory fallback is used automatically

## Roadmap (Indicative)

- Inline citation markers [1][2] with copy/open
- Conversation persistence and export (JSON/Markdown)
- Light theme toggle and accessibility polish
- Optional recruiter â€œResume Screeningâ€ and â€œOnboardingâ€ tools

---

For full technical detail, see `docs/HOW_IT_WORKS.md` and `docs/FEATURE_HR_POLICY_RAG.md`.

# HR Agent MCP Project

An HR Assistant that answers policy questions from your documents. It retrieves relevant sections from a vector database and generates grounded responses with citations via OpenAI or Gemini. Falls back to a Basic (retrievalâ€‘only) mode if no provider is configured.

## At a Glance

- Grounded answers with citations (RAG)
- Providers: OpenAI, Gemini, or Auto (prefer â†’ fallback)
- Lowâ€‘latency mode (faster models, fewer tokens)
- Summary caching per session (Redis or inâ€‘memory)
- Sticky input + streaming responses for fast UX

Quick start (Windows PowerShell):

```powershell
python -m venv .venv; .\.venv\Scripts\activate
pip install -r requirements.txt
set OPENAI_API_KEY=your-openai-key  # or set GEMINI_API_KEY
python setup.py
python scripts/warmup.py   # optional
streamlit run ui/streamlit_app.py
```

Docs: see `docs/HOW_IT_WORKS.md` (architecture & data flow) and `docs/FEATURE_HR_POLICY_RAG.md` (featureâ€‘specific guide).

## ğŸ—ï¸ Architecture

Based on your comprehensive description, here's an architecture image that visualizes your proposed AI Agent Platform. I've focused on clarity, modularity, and highlighting the shared infrastructure as well as the distinct feature pipelines.

```mermaid
flowchart LR
  subgraph UI
    ChatUI["Employee Chat UI\n(Slack/MS Teams/Web)"]
    RecruiterUI["Recruiter Dashboard\n(React)"]
    AdminUI[HR Admin Console]
  end

  subgraph MCP_Server["MCP Server / Orchestrator"]
    Manifest[Manifest + Tool Registry]
    Router[Call Router & Auth]
    Audit[Audit Logger]
  end

  subgraph Tools["MCP Tools (microservices)"]
    PolicyTool["policy_search_tool\n(vector DB + retrieval)"]
    ResumeTool["resume_screening_tool\n(ingest, embeddings, ranker)"]
    OnboardTool["onboarding_tool\n(workflows + state)"]
    Authz["authz_service\n(SSO/RBAC)"]
    Storage["object_store\n(documents, resumes)"]
    VectorDB["Vector DB\n(Chroma/Pinecone/Milvus)"]
    SQL["Postgres\n(metadata)"]
    LLMService["LLM Adapter\n(OpenAI / private)"]
    TaskQueue["Job Queue\n(Celery/Prefect)"]
    IT_System["IT / HRIS / Ticketing\n(SAP/Workday/JIRA)"]
  end

  ChatUI -->|user queries| MCP_Server
  RecruiterUI -->|requests ranking| MCP_Server
  AdminUI -->|upload docs| MCP_Server

  MCP_Server --> Router
  Router --> PolicyTool
  Router --> ResumeTool
  Router --> OnboardTool
  Router --> Authz
  Router --> Audit

  PolicyTool --> VectorDB
  PolicyTool --> Storage
  ResumeTool --> VectorDB
  ResumeTool --> Storage
  ResumeTool --> SQL
  OnboardTool --> SQL
  OnboardTool --> IT_System
  PolicyTool --> LLMService
  ResumeTool --> LLMService
  OnboardTool --> LLMService

  TaskQueue -->|background tasks| VectorDB
  TaskQueue --> Storage
  Authz --> SQL
  Audit --> SQL

  %% Styling with black text
  style ChatUI fill:#A9DFBF,stroke:#27AE60,stroke-width:2px,color:#000000
  style RecruiterUI fill:#F9E79F,stroke:#F4D03F,stroke-width:2px,color:#000000
  style AdminUI fill:#FADBD8,stroke:#C0392B,stroke-width:2px,color:#000000

  style Manifest fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000
  style Router fill:#E8F8F5,stroke:#28B463,stroke-width:2px,color:#000000
  style Audit fill:#EAF2F8,stroke:#5D6D7E,stroke-width:2px,color:#000000

  style PolicyTool fill:#D4EFDF,stroke:#2ECC71,stroke-width:2px,color:#000000
  style ResumeTool fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
  style OnboardTool fill:#EBF5FB,stroke:#3498DB,stroke-width:2px,color:#000000
  style Authz fill:#EBDEF0,stroke:#AF7AC5,stroke-width:2px,color:#000000
  style Storage fill:#F5EEF8,stroke:#9B59B6,stroke-width:2px,color:#000000
  style VectorDB fill:#D5F5E3,stroke:#2ECC71,stroke-width:2px,color:#000000
  style SQL fill:#E8F8F5,stroke:#28B463,stroke-width:2px,color:#000000
  style LLMService fill:#FADBD8,stroke:#E74C3C,stroke-width:2px,color:#000000
  style TaskQueue fill:#FCF3CF,stroke:#F4D03F,stroke-width:2px,color:#000000
  style IT_System fill:#DDEBF1,stroke:#5DADE2,stroke-width:2px,color:#000000
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- OpenAI API key

### Option 1: Automated Setup (Windows)

```bash
.\quick_start.bat
```

### Option 2: Automated Setup (Linux/Mac)

```bash
chmod +x quick_start.sh
./quick_start.sh
```

### Option 3: Manual Setup

1. **Clone and setup environment:**

   ```bash
   python -m venv venv
   # Windows:
   venv\Scripts\activate
   # Linux/Mac:
   source venv/bin/activate
   ```

2. **Install dependencies:**

   ```bash
   pip install -r requirements.txt
   ```

3. **Set OpenAI API key:**

   ```bash
   # Windows:
   set OPENAI_API_KEY=your-key-here
   # Linux/Mac:
   export OPENAI_API_KEY='your-key-here'
   ```

4. **Initialize the system:**

   ```bash
   python setup.py
   ```

5. **Start the HR Assistant:**
   ```bash
   streamlit run ui/streamlit_app.py
   ```

## ğŸ“ Project Structure

```
mcp-agent/
â”œâ”€â”€ mcp_server/           # MCP orchestration server
â”‚   â””â”€â”€ server.py        # Main MCP server with routing
â”œâ”€â”€ tools/               # MCP tools (microservices)
â”‚   â”œâ”€â”€ policy_rag/      # HR Policy RAG agent
â”‚   â”‚   â”œâ”€â”€ document_processor.py
â”‚   â”‚   â”œâ”€â”€ vector_database.py
â”‚   â”‚   â”œâ”€â”€ mcp_tool.py
â”‚   â”‚   â””â”€â”€ rag_engine.py
â”‚   â”œâ”€â”€ resume_screening/ # (Future: Resume screening)
â”‚   â””â”€â”€ onboarding/      # (Future: Onboarding agent)
â”œâ”€â”€ data/                # Document storage
â”‚   â”œâ”€â”€ hr_documents/    # HR policy documents
â”‚   â””â”€â”€ vector_db/       # Chroma vector database
â”œâ”€â”€ ui/                  # User interfaces
â”‚   â””â”€â”€ streamlit_app.py # Main chat interface
â”œâ”€â”€ docs/                # Documentation
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ setup.py            # System initialization
â””â”€â”€ .env.example        # Environment configuration
```

## ğŸ¯ Features Implemented

### âœ… Feature 1: HR Policy RAG Agent (MVP)

**What it does:**

- Answers employee HR questions using company policy documents
- Provides grounded responses with citations
- Maintains conversation context
- Real-time vector similarity search

**Components:**

- **Document Processor**: Converts PDFs/text to searchable chunks
- **Vector Database**: Chroma DB for semantic search
- **MCP Tool**: `policy_search` for document retrieval
- **RAG Engine**: OpenAI-powered response generation
- **Chat UI**: Streamlit interface for employees

**Example queries:**

- "How many vacation days do I get?"
- "What's the remote work policy?"
- "How do I request sick leave?"
- "What health benefits are available?"

### ğŸ”® Planned Features

**Feature 2: Resume Screening Agent**

- Rank candidate resumes against job descriptions
- Semantic matching and skill extraction
- Bias mitigation and fair screening

**Feature 3: Onboarding Agent**

- Guide new hires through onboarding tasks
- Checklist management and progress tracking
- Integration with IT/HR systems

## ğŸ› ï¸ Technical Details

### MCP (Model Context Protocol) Architecture

- **Server**: Central orchestrator handling tool calls and routing
- **Tools**: Microservices for specific HR functions
- **Router**: Authentication, authorization, and audit logging
- **Manifest**: Tool registry and capability discovery

### RAG (Retrieval Augmented Generation)

- **Embeddings**: Sentence-transformers for semantic search
- **Chunking**: Smart text splitting with overlap for context
- **Retrieval**: Vector similarity search in Chroma DB
- **Generation**: OpenAI GPT for grounded responses with citations

### Data Flow

```
User Query â†’ MCP Server â†’ policy_search Tool â†’ Vector DB â†’
Relevant Chunks â†’ RAG Engine â†’ LLM â†’ Cited Response â†’ UI
```

## ğŸ“Š System Status

The setup script will show:

- âœ… Document processing status
- âœ… Vector database initialization
- âœ… MCP server health check
- âœ… Tool registration confirmation

## ğŸ”§ Configuration

### Environment Variables (`.env`)

```bash
OPENAI_API_KEY=your_openai_api_key_here
VECTOR_DB_PATH=./data/vector_db
VECTOR_DB_COLLECTION_NAME=hr_policies
HR_DOCUMENTS_PATH=./data/hr_documents
EMBEDDING_MODEL=all-MiniLM-L6-v2
# Optional: Gemini
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-1.5-pro

# Redis cache (for conversation summaries)
REDIS_URL=redis://localhost:6379/0
# or
# REDIS_HOST=localhost
# REDIS_PORT=6379
# REDIS_DB=0

# Low-latency tuning
FAST_OPENAI_MODEL=gpt-3.5-turbo
FAST_GEMINI_MODEL=gemini-1.5-flash
LOW_LATENCY_MAX_TOKENS=350
LOW_LATENCY_TEMPERATURE=0.0
# Force basic-only in low-latency (fallback search)
# LOW_LATENCY_BASIC_ONLY=true
```

### Adding Your Own HR Documents

1. Place PDF or text files in `data/hr_documents/`
2. Run: `python setup.py` to reprocess
3. Restart the UI: `streamlit run ui/streamlit_app.py`

## ğŸ§ª Testing

### Test Individual Components

```bash
# Test vector database
python tools/policy_rag/vector_database.py

# Test MCP server
python mcp_server/server.py

# Test RAG engine
python tools/policy_rag/rag_engine.py
```

### Test End-to-End Flow

1. Start the UI: `streamlit run ui/streamlit_app.py`
2. Ask a question: "How many vacation days do I get?"
3. Verify response includes citations
4. Check debug info in sidebar

## âš¡ Warm-up (reduce first-response latency)

Run the warm-up script before starting Streamlit to pre-initialize the vector index and the LLM provider:

```powershell
python scripts/warmup.py
```

Then start the app:

```powershell
streamlit run ui/streamlit_app.py
```

## ğŸ—ƒï¸ Caching

- Conversation summaries are cached in Redis per session (`conv:summary:<user_id>`) for ~30 minutes.
- If Redis is not available, an in-memory fallback is used (per-process).
- Summaries shrink the context sent to the model by combining a compact summary with just the latest turns.

## ğŸ“š Sample Data

The setup creates sample HR documents covering:

- **Employee Handbook**: Leave policies, attendance, benefits
- **IT Policies**: Computer usage, security, data protection
- **Benefits Guide**: Health insurance, retirement, wellness

## ğŸ” Troubleshooting

### Common Issues

1. **"OpenAI client not available"**

   - Install OpenAI: `pip install openai`
   - Set API key: `export OPENAI_API_KEY='your-key'`

2. **"No documents found"**

   - Run setup: `python setup.py`
   - Check `data/hr_documents/` has files

3. **"Import errors"**

   - Install dependencies: `pip install -r requirements.txt`
   - Check Python version (3.8+)

4. **"Vector DB empty"**
   - Reinitialize: `python setup.py`
   - Check document processing logs

### Getting Help

- Check system status in UI sidebar
- Review logs for error details
- Test components individually

## ğŸ‰ Success Indicators

âœ… **Working System Shows:**

- Green health status in UI sidebar
- Database stats showing processed chunks
- Search results with citations
- LLM responses with document references
- Conversation history maintained

## ğŸš§ Development Guidelines

Following the project rules in `docs/things-to-keep-in-mind.md`:

- âœ… One feature at a time (HR Policy RAG first)
- âœ… Keep main branch clean and demo-ready
- âœ… Start small, test immediately
- âœ… Clear file structure and readable code
- âœ… Independent features with minimal coupling

## ğŸ“ˆ Next Steps

1. **Deploy Feature 1** to production
2. **Add Feature 2**: Resume screening agent
3. **Add Feature 3**: Onboarding workflow agent
4. **Enhance UI**: Better styling, mobile support
5. **Add Authentication**: SSO integration
6. **Scale Infrastructure**: Production databases, monitoring

---

**HR Agent MCP - Making HR knowledge accessible to everyone** ğŸ¯
